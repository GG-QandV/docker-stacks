services:
  llm-service:
    image: ghcr.io/ggml-org/llama.cpp:server
    container_name: llm-service
    ports:
      - "8085:8080"
    volumes:
      - /home/gg/orchestrator/models/universal/Qwen:/models:ro
    command: >
      -m /models/Qwen2.5-7B-Instruct-Q4_K_M.gguf
      --host 0.0.0.0
      --port 8080
      -c 1024
      -t 3
      -ub 128
    restart: unless-stopped
networks:
  orchestrator-network:
    name: orchestrator-network
    external: true
