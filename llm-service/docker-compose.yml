version: '3.8'
services:
  llm-standalone:
    image: ghcr.io/ggerganov/llama.cpp:server
    ports:
      - "8085:8080"
    volumes:
      - ~/orchestrator/models/universal:/models:ro
    environment:
      - HOST=0.0.0.0
      - PORT=8080
      - LLAMA_ARG_NO_MMAP=True  # Исправление exit 139
    command: >
      -m /models/DeepSeek/DeepSeek-R1-Distill-Llama-8B-Q4_K_M.gguf
      --host 0.0.0.0
      --port 8080
      -c 2048
      -t 4
      --no-mmap
    deploy:
      resources:
        limits:
          memory: 2500M
        reservations:
          memory: 2000M
    restart: unless-stopped

