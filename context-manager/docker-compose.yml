
services:
  context-manager:
    build: .
    container_name: context-manager
    ports:
      - "3847:3847"
    environment:
      - DATABASE_URL=postgresql://postgres:Mart436780@postgresql-postgres-main-1:5432/context_db
      - QDRANT_HOST=qdrant-new
      - QDRANT_PORT=6333
      - TEI_HOST=http://tei-embeddings:80
      - EMBEDDING_PROVIDER=huggingface-tei
      - EMBEDDING_DIMENSIONS=384
    networks:
      - orchestrator-network
    extra_hosts:
      - "host.docker.internal:host-gateway"
    mem_limit: 512m
    depends_on:
      - tei-embeddings
    restart: unless-stopped

  tei-embeddings:
    image: ghcr.io/huggingface/text-embeddings-inference:cpu-1.8.3
    pull_policy: if_not_present
    container_name: tei-service
    shm_size: 1024m
    mem_limit: 1536m
    mem_reservation: 768m
    ports:
      - "8080:80"
    networks:
      - orchestrator-network
    volumes:
      - /home/gg/orchestrator/models/embedding/multilingual-e5-small_Q8/onnx:/data:ro
    environment:
      - RAYON_NUM_THREADS=2
      - OMP_NUM_THREADS=2
      - MALLOC_ARENA_MAX=2
      - HF_HUB_OFFLINE=1
      - OMP_WAIT_POLICY=PASSIVE
    command: >
      --model-id /data
      --pooling mean
      --tokenization-workers 2
      --max-batch-tokens 512
      --max-concurrent-requests 2
      --auto-truncate
    restart: unless-stopped

networks:
  orchestrator-network:
    name: orchestrator-network
    external: true

